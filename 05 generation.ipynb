{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到了整个流程的最后一个环节：生成答案。在这一步，大语言模型将下场，根据输入的提示词生成一个输出作为答案。由于模型比较大，在这个项目中使用 Ollama 作为模型服务。Ollama 使用 4bit 量化模型，大幅度减小了资源消耗和占用；同时由于大语言模型性能的提高，小模型就已经可堪大用。所以在这个系列中，我们使用 Qwen 2.5 0.5B 模型。Ollama 的安装方法在系列开篇中已经介绍过了，这里不再赘述。另外本篇还会介绍如何使用 GPT 作为后端语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenjiazhai/miniconda3/envs/langchain/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
    "\n",
    "root = os.getcwd()\n",
    "loader = TextLoader(os.path.join(root, 'data/paul_graham/paul_graham_essay.txt'))\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "vector_store = FAISS.from_documents(docs, embedding)\n",
    "\n",
    "query = 'What did the author do growing up?'\n",
    "res = vector_store.similarity_search(query, k=3)\n",
    "context = '\\n'.join([doc.page_content for doc in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造提示词\n",
    "所谓“提示词”就是大语言模型的输入。提示词的质量直接决定了模型生成的文本质量。如何构建大语言模型的提示词是一个重要的研究方向，并且因模型而异，需要不断尝试、调整。一般而言，有以下几个原则：\n",
    "- 提示词要尽可能清楚，目标明确。\n",
    "- 提示词内最好有角色分工，比如用户的输入可以是以 \"user\" 的身份提出的，任务的信息可以在 \"system\" 中给出，模型生成的答案也可以是 \"assistant\" 的身份。\n",
    "- 可以给 AI 立一个人设，高大上的人设在某些时候可能会有正收益，比如“资深记着”、“经验丰富的专家”、“拥有丰富知识的老师”等等。\n",
    "\n",
    "LangChain 中有关于提示词的功能，现在先手写一个简单的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''\n",
    "You are a helpful assistant.\n",
    "Answer the question based on the following context:\n",
    "{context}\n",
    "Question: {query}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK，我们已经把准备工作做好了。然后干嘛呢？下面分别介绍 Ollama 和 GPT 的调用方法。\n",
    "# Ollama\n",
    "其实 Ollama 有两个类可以生成文本，一个是 `ChatOllama`，另一个是 `OllamaLLM`。这两个类的参数很像，但是 `ChatOllama` 是一个聊天模型， `OllamaLLM` 是一个文本生成模型。他们相同的主要参数有：\n",
    "- `model`：模型名称\n",
    "- `temperature`：温度，默认为 0.8\n",
    "- `top_k`： 从前 k 个概率最大的词中采样，默认为 40\n",
    "- `num_predict`：最大输出长度，默认为 128\n",
    "\n",
    "假设已经载入了 Ollama 模型，下面可以直接推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the context provided, the author grew up primarily by working outside of school. They focused on writing and programming during college, which was before they started their career in coding or software development. While they mentioned having essays about various topics written while at university, it seems that these were not published as books yet.\\n\\nAdditionally, throughout their growing-up years, the author wrote essays and worked on projects related to their interests outside of school. They described writing short stories, and then went on to describe projects such as programming code, doing painting, reading \"Hackers & Painters,\" making dinner parties for friends, and buying a building in Cambridge. These activities clearly demonstrate how they found themselves growing up while still pursuing their academic and personal goals.\\n\\nThe author\\'s life outside of school appears to have been focused on creative pursuits rather than the traditional academic path recommended by their parents or by society at large.', additional_kwargs={}, response_metadata={'model': 'qwen2.5:0.5b', 'created_at': '2024-11-13T08:50:14.566539Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2632809791, 'load_duration': 612677125, 'prompt_eval_count': 289, 'prompt_eval_duration': 453000000, 'eval_count': 180, 'eval_duration': 1331000000}, id='run-5fc8569b-1650-44a8-ab2f-0e8357ec6899-0', usage_metadata={'input_tokens': 289, 'output_tokens': 180, 'total_tokens': 469})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"qwen2.5:0.5b\")\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `OllamaLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the given context, the author worked outside of school before college by writing and programming stories. These activities were part of their personal interests during the time they weren't enrolled in formal educational institutions or attending universities. The text describes these experiences as a way for the author to help them learn something about programming and storytelling.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:0.5b\")\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，`ChatOllama` 返回了一个 `AIMessage` 对象，而 `OllamaLLM` 直接返回了一个字符串。前者在处理复杂任务时可以提供更大的可定制性，而后者则更易于使用。\n",
    "# OpenAI\n",
    "与 Ollama 相似，langchain_openai 也提供了 `ChatOpenAI` 和 `OpenAI` 类。它们的主要参数有：\n",
    "- `model`：模型名称\n",
    "- `temperature`：模型的温度\n",
    "- `max_tokens`：最大 token 生成数量\n",
    "- `logprobs`：是否返回概率的对数\n",
    "\n",
    "由于这个库是一个远程库，还提供了一些客户端参数：\n",
    "- `api_key`：API 密钥\n",
    "- `timeout`：请求超时时间\n",
    "- `base_url`：API 的 URL\n",
    "- `max_retries`：最大重试次数\n",
    "\n",
    "限于篇幅，这里只展示 `ChatOpenAI` 类的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, api_key=\"\", model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Growing up, the author worked on writing and programming. They wrote short stories, which they acknowledged were not very good, and later wrote essays on various topics. Additionally, they engaged in cooking, hosting dinners for a group of friends, and took on projects like working on spam filters and painting.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 260, 'total_tokens': 319, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-166e4384-92f4-4056-8721-8332bf34a98b-0' usage_metadata={'input_tokens': 260, 'output_tokens': 59, 'total_tokens': 319, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
